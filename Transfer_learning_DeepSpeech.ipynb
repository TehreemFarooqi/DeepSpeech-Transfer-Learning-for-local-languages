{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tarjumaan_DeepSpeech.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z7I9AA5udz7l"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jozykLxkFzGe",
        "outputId": "b80624e1-933c-4914-e7a3-376f153ff0fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cj2PufDdgmp"
      },
      "source": [
        "# Downloading code and installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKlkTEKynKSZ",
        "outputId": "4ce90784-c9b2-4268-effb-6f14b1163c40"
      },
      "source": [
        "!git clone --branch v0.9.3 https://github.com/mozilla/DeepSpeech\n",
        "\n",
        "#!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-checkpoint.tar.gz\n",
        "#!tar -xvf deepspeech-0.9.3-checkpoint.tar.gz -C /content/drive/MyDrive/scratch_model/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepSpeech'...\n",
            "remote: Enumerating objects: 23874, done.\u001b[K\n",
            "remote: Counting objects: 100% (411/411), done.\u001b[K\n",
            "remote: Compressing objects: 100% (185/185), done.\u001b[K\n",
            "remote: Total 23874 (delta 231), reused 358 (delta 213), pack-reused 23463\u001b[K\n",
            "Receiving objects: 100% (23874/23874), 49.48 MiB | 22.48 MiB/s, done.\n",
            "Resolving deltas: 100% (16362/16362), done.\n",
            "Note: checking out 'f2e9c85880dff94115ab510cde9ca4af7ee51c19'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJDCLLO4nftI",
        "outputId": "b98a9c8e-c464-45e2-de40-74df676d78a7"
      },
      "source": [
        "%cd /content/\n",
        "!sudo apt-get install python3-venv\n",
        "!sudo apt-get install python3-dev\n",
        "!pip install --upgrade pip\n",
        "!sudo apt-get install sox\n",
        "!sudo apt-get install sox libsox-fmt-mp3\n",
        "!sudo apt install git\n",
        "!pip install librosa==0.7.2\n",
        "!sudo apt-get install pciutils\n",
        "!lspci | grep -i nvidia\n",
        "!pip3 install deepspeech-gpu==0.9.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3.6-venv\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-venv python3.6-venv\n",
            "0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 1,660 kB of archives.\n",
            "After this operation, 1,902 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.4 [1,653 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3.6-venv amd64 3.6.9-1~18.04ubuntu1.4 [6,188 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-venv amd64 3.6.7-1~18.04 [1,208 B]\n",
            "Fetched 1,660 kB in 1s (1,214 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../python-pip-whl_9.0.1-2.3~ubuntu1.18.04.4_all.deb ...\n",
            "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Selecting previously unselected package python3.6-venv.\n",
            "Preparing to unpack .../python3.6-venv_3.6.9-1~18.04ubuntu1.4_amd64.deb ...\n",
            "Unpacking python3.6-venv (3.6.9-1~18.04ubuntu1.4) ...\n",
            "Selecting previously unselected package python3-venv.\n",
            "Preparing to unpack .../python3-venv_3.6.7-1~18.04_amd64.deb ...\n",
            "Unpacking python3-venv (3.6.7-1~18.04) ...\n",
            "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Setting up python3.6-venv (3.6.9-1~18.04ubuntu1.4) ...\n",
            "Setting up python3-venv (3.6.7-1~18.04) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "python3-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/6f/43037c7bcc8bd8ba7c9074256b1a11596daa15555808ec748048c1507f08/pip-21.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 8.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.1.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox3\n",
            "Suggested packages:\n",
            "  file libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox3 sox\n",
            "0 upgraded, 8 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 760 kB of archives.\n",
            "After this operation, 6,717 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrnb0 amd64 0.1.3-2.1 [92.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrwb0 amd64 0.1.3-2.1 [45.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox3 amd64 14.4.2-3ubuntu0.18.04.1 [226 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2-3ubuntu0.18.04.1 [10.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-base amd64 14.4.2-3ubuntu0.18.04.1 [32.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 sox amd64 14.4.2-3ubuntu0.18.04.1 [101 kB]\n",
            "Fetched 760 kB in 1s (598 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 8.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 160744 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../2-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../3-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../4-libsox3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../5-libsox-fmt-alsa_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../6-libsox-fmt-base_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../7-sox_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "sox is already the newest version (14.4.2-3ubuntu0.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libid3tag0 libmad0\n",
            "The following NEW packages will be installed:\n",
            "  libid3tag0 libmad0 libsox-fmt-mp3\n",
            "0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 112 kB of archives.\n",
            "After this operation, 370 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libid3tag0 amd64 0.15.1b-13 [31.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libmad0 amd64 0.15.1b-9ubuntu18.04.1 [64.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2-3ubuntu0.18.04.1 [15.9 kB]\n",
            "Fetched 112 kB in 1s (136 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "(Reading database ... 160832 files and directories currently installed.)\n",
            "Preparing to unpack .../libid3tag0_0.15.1b-13_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../libmad0_0.15.1b-9ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../libsox-fmt-mp3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.8).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting librosa==0.7.2\n",
            "  Downloading librosa-0.7.2.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.2) (2.1.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.2) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.2) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.2) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.2) (1.0.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.2) (4.4.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.2) (1.15.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.2) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.2) (0.51.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.2) (0.10.3.post1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.7.2) (56.1.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.7.2) (0.34.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)\n",
            "Building wheels for collected packages: librosa\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.2-py3-none-any.whl size=1612883 sha256=53deba6743ab8117d5ae1a85570b27ec890261ead9dcda535b0cc15b8a88c409\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/9e/42/3224f85730f92fa2925f0b4fb6ef7f9c5431a64dfc77b95b39\n",
            "Successfully built librosa\n",
            "Installing collected packages: librosa\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.0\n",
            "    Uninstalling librosa-0.8.0:\n",
            "      Successfully uninstalled librosa-0.8.0\n",
            "Successfully installed librosa-0.7.2\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libpci3\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pciutils\n",
            "0 upgraded, 2 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 281 kB of archives.\n",
            "After this operation, 1,430 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpci3 amd64 1:3.5.2-1ubuntu1.1 [24.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 pciutils amd64 1:3.5.2-1ubuntu1.1 [257 kB]\n",
            "Fetched 281 kB in 1s (290 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "(Reading database ... 160854 files and directories currently installed.)\n",
            "Preparing to unpack .../libpci3_1%3a3.5.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.5.2-1ubuntu1.1) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.5.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking pciutils (1:3.5.2-1ubuntu1.1) ...\n",
            "Setting up libpci3:amd64 (1:3.5.2-1ubuntu1.1) ...\n",
            "Setting up pciutils (1:3.5.2-1ubuntu1.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "00:04.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1)\n",
            "Collecting deepspeech-gpu==0.9.3\n",
            "  Downloading deepspeech_gpu-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 156 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from deepspeech-gpu==0.9.3) (1.19.5)\n",
            "Installing collected packages: deepspeech-gpu\n",
            "Successfully installed deepspeech-gpu-0.9.3\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OTzYyONVou2c",
        "outputId": "e3843b2b-600f-4fac-d30b-818b19465f40"
      },
      "source": [
        "%cd /content/DeepSpeech\n",
        "!pip3 install folium==0.2.1\n",
        "!pip3 install --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3\n",
        "!pip3 install ds_ctcdecoder==0.9.3\n",
        "!pip3 install --upgrade --force-reinstall -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech\n",
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.0)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79978 sha256=de11bdd1b687c7bfcb5081fe248f1d5ef66001e08e70fdff6b18c749c73bd657\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting pip==20.0.2\n",
            "  Downloading pip-20.0.2-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 8.1 MB/s \n",
            "\u001b[?25hCollecting wheel==0.34.2\n",
            "  Downloading wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
            "Collecting setuptools==46.1.3\n",
            "  Downloading setuptools-46.1.3-py3-none-any.whl (582 kB)\n",
            "\u001b[K     |████████████████████████████████| 582 kB 94.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: wheel, setuptools, pip\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.36.2\n",
            "    Uninstalling wheel-0.36.2:\n",
            "      Successfully uninstalled wheel-0.36.2\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 56.1.0\n",
            "    Uninstalling setuptools-56.1.0:\n",
            "      Successfully uninstalled setuptools-56.1.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.1\n",
            "    Uninstalling pip-21.1.1:\n",
            "      Successfully uninstalled pip-21.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.4.1 requires wheel~=0.35, but you have wheel 0.34.2 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-20.0.2 setuptools-46.1.3 wheel-0.34.2\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting ds_ctcdecoder==0.9.3\n",
            "  Downloading ds_ctcdecoder-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 10.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from ds_ctcdecoder==0.9.3) (1.19.5)\n",
            "Installing collected packages: ds-ctcdecoder\n",
            "Successfully installed ds-ctcdecoder-0.9.3\n",
            "Obtaining file:///content/DeepSpeech\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 288 kB/s \n",
            "\u001b[?25hCollecting progressbar2\n",
            "  Downloading progressbar2-3.53.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pyxdg\n",
            "  Downloading pyxdg-0.27-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting absl-py\n",
            "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 97.0 MB/s \n",
            "\u001b[?25hCollecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting opuslib==2.0.0\n",
            "  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-2.7.0-py3-none-any.whl (293 kB)\n",
            "\u001b[K     |████████████████████████████████| 293 kB 61.4 MB/s \n",
            "\u001b[?25hCollecting sox\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 57.7 MB/s \n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 9.8 MB/s \n",
            "\u001b[?25hCollecting numba==0.47.0\n",
            "  Downloading numba-0.47.0-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 90.9 MB/s \n",
            "\u001b[?25hCollecting llvmlite==0.31.0\n",
            "  Downloading llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 107 kB/s \n",
            "\u001b[?25hCollecting librosa\n",
            "  Downloading librosa-0.8.0.tar.gz (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 99.3 MB/s \n",
            "\u001b[?25hCollecting soundfile\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting ds_ctcdecoder==0.9.3\n",
            "  Using cached ds_ctcdecoder-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "Collecting tensorflow==1.15.4\n",
            "  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 21 kB/s \n",
            "\u001b[?25hCollecting python-utils>=2.3.0\n",
            "  Downloading python_utils-2.5.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.7.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.7 MB/s \n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "  Downloading alembic-1.6.2.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 79.4 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
            "Collecting scipy!=1.4.0\n",
            "  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 123.5 MB/s \n",
            "\u001b[?25hCollecting sqlalchemy>=1.1.0\n",
            "  Downloading SQLAlchemy-1.4.15-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 48.3 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 96.5 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.7.3\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 101.7 MB/s \n",
            "\u001b[?25hCollecting pytz>=2017.3\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 71.0 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 81.4 MB/s \n",
            "\u001b[?25hCollecting chardet<5,>=3.0.2\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 106.9 MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 78.6 MB/s \n",
            "\u001b[?25hCollecting idna<3,>=2.5\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting setuptools\n",
            "  Downloading setuptools-56.2.0-py3-none-any.whl (785 kB)\n",
            "\u001b[K     |████████████████████████████████| 785 kB 57.4 MB/s \n",
            "\u001b[?25hCollecting audioread>=2.0.0\n",
            "  Downloading audioread-2.1.9.tar.gz (377 kB)\n",
            "\u001b[K     |████████████████████████████████| 377 kB 99.2 MB/s \n",
            "\u001b[?25hCollecting scikit-learn!=0.19.0,>=0.14.0\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting joblib>=0.14\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 93.0 MB/s \n",
            "\u001b[?25hCollecting decorator>=3.0.0\n",
            "  Downloading decorator-5.0.9-py3-none-any.whl (8.9 kB)\n",
            "Collecting resampy>=0.2.2\n",
            "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
            "\u001b[K     |████████████████████████████████| 323 kB 95.5 MB/s \n",
            "\u001b[?25hCollecting pooch>=1.0\n",
            "  Downloading pooch-1.3.0-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 537 kB/s \n",
            "\u001b[?25hCollecting cffi>=1.0\n",
            "  Downloading cffi-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 83.9 MB/s \n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 60.8 MB/s \n",
            "\u001b[?25hCollecting wheel>=0.26; python_version >= \"3\"\n",
            "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 76.0 MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.6.1\n",
            "  Downloading protobuf-3.17.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 82.6 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting keras-preprocessing>=1.0.5\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.38.0-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 68.5 MB/s \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting PyYAML>=3.12\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 57.3 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 96.2 MB/s \n",
            "\u001b[?25hCollecting PrettyTable>=0.7.2\n",
            "  Downloading prettytable-2.1.0-py3-none-any.whl (22 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-1.5.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 95.4 MB/s \n",
            "\u001b[?25hCollecting pyparsing>=2.1.0\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting greenlet!=0.4.17; python_version >= \"3\"\n",
            "  Downloading greenlet-1.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 101.1 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\"\n",
            "  Downloading importlib_metadata-4.0.1-py3-none-any.whl (16 kB)\n",
            "Collecting soupsieve>1.2; python_version >= \"3.0\"\n",
            "  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting appdirs\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting pycparser\n",
            "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 95.1 MB/s \n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 83.8 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 104.4 MB/s \n",
            "\u001b[?25hCollecting wcwidth\n",
            "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Collecting attrs>=16.3.0\n",
            "  Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting MarkupSafe>=0.9.2\n",
            "  Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux2010_x86_64.whl (31 kB)\n",
            "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
            "  Downloading typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
            "Collecting cached-property; python_version < \"3.8\"\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Building wheels for collected packages: opuslib, bs4, librosa, alembic, audioread, resampy, termcolor, gast, wrapt, pyperclip\n",
            "  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=85660c17c2bb386b21179dd8be3860cd9bfdb5b58cd441fd6c46502219e1882d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/ba/d4/0e81231a9797fbb262ae3a54fd761fab850db7f32d94a3283a\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=7706a407cf6d20006c2bbf83e40167cc1291d0b146ecdcf9244ce9b67cd4f365\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/9e/ba/20e5bbc1afef3a491f0b3bb74d508f99403aabe76eda2167ca\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.8.0-py3-none-any.whl size=201374 sha256=3bec452923f9844c10130c3d5e5d85bd0faa806d78c343aeb05f9f0c8c52f496\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/1e/aa/d91797ae7e1ce11853ee100bee9d1781ae9d750e7458c95afb\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.6.2-py2.py3-none-any.whl size=164220 sha256=108407e09a8a41d165d762fd4e5b2ccaca2144a4991ad7657be1dee0f0fc2db2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/f8/80/03e817e451a772440cf9f83f5e91a58591757c9904592e96bc\n",
            "  Building wheel for audioread (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for audioread: filename=audioread-2.1.9-py3-none-any.whl size=23142 sha256=c2e66bbc4b6d15efecd8ca88575e1a2fb5d300a82cef0150fd3c43e610bf5321\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/7b/eb/213741ccc0678f63e346ab8dff10495995ca3f426af87b8d88\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320720 sha256=ff7affc1a8bfe722b10b9c9d1db6a2a58a9b3d6e374cb58009f3baadf32d81cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/18/0a/8ad18a597d8333a142c9789338a96a6208f1198d290ece356c\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=df02f3aa957b20073dcaeb7e7773f5f8893f42d9b33f2565c4b0fdafbeddf01e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=5ff390b357a6a89d84ecad006b6e069b9b2cb81cd4521037c8181a252720e49e\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68669 sha256=76ad037261fc15f10b83ec3accf158fcee6da08f40df53658f3652d42f72f033\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11107 sha256=2b04c65016dc0a33e6682453a5b6b77ffb51bb1d0053ac027ff423f569b8fc57\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built opuslib bs4 librosa alembic audioread resampy termcolor gast wrapt pyperclip\n",
            "\u001b[31mERROR: tensorflow 1.15.4 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.20.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: networkx 2.5.1 has requirement decorator<5,>=4.3, but you'll have decorator 5.0.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: moviepy 0.2.3.5 has requirement decorator<5.0,>=4.0.2, but you'll have decorator 5.0.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.16.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, six, python-utils, progressbar2, pyxdg, attrdict, absl-py, semver, opuslib, PyYAML, pbr, wcwidth, typing-extensions, zipp, importlib-metadata, PrettyTable, stevedore, pyperclip, attrs, colorama, cmd2, pyparsing, cliff, packaging, greenlet, sqlalchemy, MarkupSafe, Mako, python-editor, python-dateutil, alembic, tqdm, cmaes, colorlog, scipy, optuna, sox, soupsieve, beautifulsoup4, bs4, pytz, pandas, urllib3, chardet, certifi, idna, requests, llvmlite, setuptools, numba, audioread, joblib, threadpoolctl, scikit-learn, decorator, resampy, pycparser, cffi, soundfile, appdirs, pooch, librosa, ds-ctcdecoder, astor, termcolor, tensorflow-estimator, wheel, cached-property, h5py, keras-applications, markdown, protobuf, grpcio, werkzeug, tensorboard, gast, keras-preprocessing, google-pasta, opt-einsum, wrapt, tensorflow, deepspeech-training\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: python-utils\n",
            "    Found existing installation: python-utils 2.5.6\n",
            "    Uninstalling python-utils-2.5.6:\n",
            "      Successfully uninstalled python-utils-2.5.6\n",
            "  Attempting uninstall: progressbar2\n",
            "    Found existing installation: progressbar2 3.38.0\n",
            "    Uninstalling progressbar2-3.38.0:\n",
            "      Successfully uninstalled progressbar2-3.38.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Attempting uninstall: semver\n",
            "    Found existing installation: semver 2.13.0\n",
            "    Uninstalling semver-2.13.0:\n",
            "      Successfully uninstalled semver-2.13.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.5\n",
            "    Uninstalling wcwidth-0.2.5:\n",
            "      Successfully uninstalled wcwidth-0.2.5\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.7.4.3\n",
            "    Uninstalling typing-extensions-3.7.4.3:\n",
            "      Successfully uninstalled typing-extensions-3.7.4.3\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.4.1\n",
            "    Uninstalling zipp-3.4.1:\n",
            "      Successfully uninstalled zipp-3.4.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.0.1\n",
            "    Uninstalling importlib-metadata-4.0.1:\n",
            "      Successfully uninstalled importlib-metadata-4.0.1\n",
            "  Attempting uninstall: PrettyTable\n",
            "    Found existing installation: prettytable 2.1.0\n",
            "    Uninstalling prettytable-2.1.0:\n",
            "      Successfully uninstalled prettytable-2.1.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 21.2.0\n",
            "    Uninstalling attrs-21.2.0:\n",
            "      Successfully uninstalled attrs-21.2.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 2.4.7\n",
            "    Uninstalling pyparsing-2.4.7:\n",
            "      Successfully uninstalled pyparsing-2.4.7\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 20.9\n",
            "    Uninstalling packaging-20.9:\n",
            "      Successfully uninstalled packaging-20.9\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 1.1.0\n",
            "    Uninstalling greenlet-1.1.0:\n",
            "      Successfully uninstalled greenlet-1.1.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 1.4.15\n",
            "    Uninstalling SQLAlchemy-1.4.15:\n",
            "      Successfully uninstalled SQLAlchemy-1.4.15\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.0\n",
            "    Uninstalling MarkupSafe-2.0.0:\n",
            "      Successfully uninstalled MarkupSafe-2.0.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.1\n",
            "    Uninstalling python-dateutil-2.8.1:\n",
            "      Successfully uninstalled python-dateutil-2.8.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Attempting uninstall: bs4\n",
            "    Found existing installation: bs4 0.0.1\n",
            "    Uninstalling bs4-0.0.1:\n",
            "      Successfully uninstalled bs4-0.0.1\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2020.12.5\n",
            "    Uninstalling certifi-2020.12.5:\n",
            "      Successfully uninstalled certifi-2020.12.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 46.1.3\n",
            "    Uninstalling setuptools-46.1.3:\n",
            "      Successfully uninstalled setuptools-46.1.3\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: audioread\n",
            "    Found existing installation: audioread 2.1.9\n",
            "    Uninstalling audioread-2.1.9:\n",
            "      Successfully uninstalled audioread-2.1.9\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.2.2\n",
            "    Uninstalling resampy-0.2.2:\n",
            "      Successfully uninstalled resampy-0.2.2\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.20\n",
            "    Uninstalling pycparser-2.20:\n",
            "      Successfully uninstalled pycparser-2.20\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.14.5\n",
            "    Uninstalling cffi-1.14.5:\n",
            "      Successfully uninstalled cffi-1.14.5\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: SoundFile 0.10.3.post1\n",
            "    Uninstalling SoundFile-0.10.3.post1:\n",
            "      Successfully uninstalled SoundFile-0.10.3.post1\n",
            "  Attempting uninstall: appdirs\n",
            "    Found existing installation: appdirs 1.4.4\n",
            "    Uninstalling appdirs-1.4.4:\n",
            "      Successfully uninstalled appdirs-1.4.4\n",
            "  Attempting uninstall: pooch\n",
            "    Found existing installation: pooch 1.3.0\n",
            "    Uninstalling pooch-1.3.0:\n",
            "      Successfully uninstalled pooch-1.3.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.7.2\n",
            "    Uninstalling librosa-0.7.2:\n",
            "      Successfully uninstalled librosa-0.7.2\n",
            "  Attempting uninstall: ds-ctcdecoder\n",
            "    Found existing installation: ds-ctcdecoder 0.9.3\n",
            "    Uninstalling ds-ctcdecoder-0.9.3:\n",
            "      Successfully uninstalled ds-ctcdecoder-0.9.3\n",
            "  Attempting uninstall: astor\n",
            "    Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.34.2\n",
            "    Uninstalling wheel-0.34.2:\n",
            "      Successfully uninstalled wheel-0.34.2\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.3.4\n",
            "    Uninstalling Markdown-3.3.4:\n",
            "      Successfully uninstalled Markdown-3.3.4\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 2.0.0\n",
            "    Uninstalling Werkzeug-2.0.0:\n",
            "      Successfully uninstalled Werkzeug-2.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Attempting uninstall: keras-preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Running setup.py develop for deepspeech-training\n",
            "Successfully installed Mako-1.1.4 MarkupSafe-2.0.1 PrettyTable-2.1.0 PyYAML-5.4.1 absl-py-0.12.0 alembic-1.6.2 appdirs-1.4.4 astor-0.8.1 attrdict-2.0.1 attrs-21.2.0 audioread-2.1.9 beautifulsoup4-4.9.3 bs4-0.0.1 cached-property-1.5.2 certifi-2020.12.5 cffi-1.14.5 chardet-4.0.0 cliff-3.7.0 cmaes-0.8.2 cmd2-1.5.0 colorama-0.4.4 colorlog-5.0.1 decorator-5.0.9 deepspeech-training ds-ctcdecoder-0.9.3 gast-0.2.2 google-pasta-0.2.0 greenlet-1.1.0 grpcio-1.38.0 h5py-3.2.1 idna-2.10 importlib-metadata-4.0.1 joblib-1.0.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 librosa-0.8.0 llvmlite-0.31.0 markdown-3.3.4 numba-0.47.0 numpy-1.20.3 opt-einsum-3.3.0 optuna-2.7.0 opuslib-2.0.0 packaging-20.9 pandas-1.2.4 pbr-5.6.0 pooch-1.3.0 progressbar2-3.53.1 protobuf-3.17.0 pycparser-2.20 pyparsing-2.4.7 pyperclip-1.8.2 python-dateutil-2.8.1 python-editor-1.0.4 python-utils-2.5.6 pytz-2021.1 pyxdg-0.27 requests-2.25.1 resampy-0.2.2 scikit-learn-0.24.2 scipy-1.6.3 semver-2.13.0 setuptools-56.2.0 six-1.16.0 soundfile-0.10.3.post1 soupsieve-2.2.1 sox-1.4.1 sqlalchemy-1.4.15 stevedore-3.3.0 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1 termcolor-1.1.0 threadpoolctl-2.1.0 tqdm-4.60.0 typing-extensions-3.10.0.0 urllib3-1.26.4 wcwidth-0.2.5 werkzeug-2.0.1 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "astor",
                  "cffi",
                  "dateutil",
                  "decorator",
                  "google",
                  "numpy",
                  "pandas",
                  "pkg_resources",
                  "pyparsing",
                  "pytz",
                  "six",
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XStiLMkNaEIY",
        "outputId": "5e9ad7b9-0220-4699-f12c-0e647078b5b4"
      },
      "source": [
        "!echo $PATH\n",
        "\n",
        "import os\n",
        "os.environ['PATH'] += \":/usr/local/cuda-10.0/bin\"\n",
        "os.environ['CUDADIR'] = \"/usr/local/cuda-10.0\"  \n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] = \"/usr/lib64-nvidia:/usr/local/cuda-10.0/lib64\"\n",
        "\n",
        "!echo $PATH\n",
        "!echo $LD_LIBRARY_PATH\n",
        "!source ~/.bashrc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin\n",
            "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin:/usr/local/cuda-10.0/bin\n",
            "/usr/lib64-nvidia:/usr/local/cuda-10.0/lib64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJsw7Y9UaKd8",
        "outputId": "5b9051a7-fc01-48a9-b229-18e4da67efa2"
      },
      "source": [
        "!env | grep -i cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LD_LIBRARY_PATH=/usr/lib64-nvidia:/usr/local/cuda-10.0/lib64\n",
            "CUDADIR=/usr/local/cuda-10.0\n",
            "LIBRARY_PATH=/usr/local/cuda/lib64/stubs\n",
            "CUDA_VERSION=11.0.3\n",
            "NVIDIA_REQUIRE_CUDA=cuda>=11.0 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 brand=tesla,driver>=450,driver<451\n",
            "PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin:/usr/local/cuda-10.0/bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeQB35PXaLYG",
        "outputId": "54aa32e3-d7c5-432a-fcb9-36d5c6fc6094"
      },
      "source": [
        "%cd /content/\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "!sudo apt-get install freeglut3 freeglut3-dev libxi-dev libxmu-dev\n",
        "!sudo apt-get install build-essential dkms\n",
        "!sudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "!sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install cuda-10-0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "--2021-05-21 02:19:58--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2940 (2.9K) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1804_10.0.130-1_amd64.deb’\n",
            "\n",
            "cuda-repo-ubuntu180 100%[===================>]   2.87K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-21 02:19:58 (204 MB/s) - ‘cuda-repo-ubuntu1804_10.0.130-1_amd64.deb’ saved [2940/2940]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libxi-dev is already the newest version (2:1.7.9-1).\n",
            "libxi-dev set to manually installed.\n",
            "libxmu-dev is already the newest version (2:1.1.2-2).\n",
            "libxmu-dev set to manually installed.\n",
            "freeglut3 is already the newest version (2.8.1-3).\n",
            "freeglut3 set to manually installed.\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "dkms is already the newest version (2.3-3ubuntu9.7).\n",
            "dkms set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Selecting previously unselected package cuda-repo-ubuntu1804.\n",
            "(Reading database ... 160876 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1804_10.0.130-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu1804 (10.0.130-1) ...\n",
            "Setting up cuda-repo-ubuntu1804 (10.0.130-1) ...\n",
            "\n",
            "Configuration file '/etc/apt/sources.list.d/cuda.list'\n",
            " ==> File on system created by you or by a script.\n",
            " ==> File also in package provided by package maintainer.\n",
            "   What would you like to do about it ?  Your options are:\n",
            "    Y or I  : install the package maintainer's version\n",
            "    N or O  : keep your currently-installed version\n",
            "      D     : show the differences between the versions\n",
            "      Z     : start a shell to examine the situation\n",
            " The default action is to keep your current version.\n",
            "*** cuda.list (Y/I/N/O/D/Z) [default=N] ? Y\n",
            "Installing new version of config file /etc/apt/sources.list.d/cuda.list ...\n",
            "Executing: /tmp/apt-key-gpghome.XjuzTHIhke/gpg.1.sh --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
            "gpg: requesting key from 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub'\n",
            "gpg: key F60F4B3D7FA2AF80: \"cudatools <cudatools@nvidia.com>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Ign:1 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Get:3 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:8 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:8 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [796 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [60.5 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [423 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,412 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,152 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [452 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,183 kB]\n",
            "Hit:20 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:21 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,583 kB]\n",
            "Get:23 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,767 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [904 kB]\n",
            "Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [41.5 kB]\n",
            "Fetched 13.1 MB in 5s (2,464 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cuda-10-0 is already the newest version (10.0.130-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 73 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKoRIhXxaWxX",
        "outputId": "854eb873-a426-4209-9171-9ec10908f520"
      },
      "source": [
        "!sudo rm /usr/local/cuda\n",
        "!sudo ln -s /usr/local/cuda-10.0 /usr/local/cuda\n",
        "%ls -l /usr/local/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 80\n",
            "drwxr-xr-x  1 root root 4096 May 21 02:05 \u001b[0m\u001b[01;34mbin\u001b[0m/\n",
            "lrwxrwxrwx  1 root root   20 May 21 02:20 \u001b[01;36mcuda\u001b[0m -> \u001b[01;34m/usr/local/cuda-10.0\u001b[0m/\n",
            "drwxr-xr-x 16 root root 4096 May  6 13:26 \u001b[01;34mcuda-10.0\u001b[0m/\n",
            "drwxr-xr-x 15 root root 4096 May  6 13:29 \u001b[01;34mcuda-10.1\u001b[0m/\n",
            "drwxr-xr-x  1 root root 4096 May  6 13:32 \u001b[01;34mcuda-11.0\u001b[0m/\n",
            "drwxr-xr-x  1 root root 4096 May 13 13:24 \u001b[01;34metc\u001b[0m/\n",
            "drwxr-xr-x  2 root root 4096 Sep 21  2020 \u001b[01;34mgames\u001b[0m/\n",
            "drwxr-xr-x  2 root root 4096 May 13 13:36 \u001b[01;34m_gcs_config_ops.so\u001b[0m/\n",
            "drwxr-xr-x  1 root root 4096 May 13 13:44 \u001b[01;34minclude\u001b[0m/\n",
            "drwxr-xr-x  1 root root 4096 May 13 13:44 \u001b[01;34mlib\u001b[0m/\n",
            "-rw-r--r--  1 root root 1636 May 13 13:39 LICENSE.txt\n",
            "drwxr-xr-x  3 root root 4096 May 13 13:36 \u001b[01;34mlicensing\u001b[0m/\n",
            "lrwxrwxrwx  1 root root    9 Sep 21  2020 \u001b[01;36mman\u001b[0m -> \u001b[01;34mshare/man\u001b[0m/\n",
            "drwxr-xr-x  2 root root 4096 Sep 21  2020 \u001b[01;34msbin\u001b[0m/\n",
            "-rw-r--r--  1 root root 7291 May 13 13:39 setup.cfg\n",
            "drwxr-xr-x  1 root root 4096 May 13 13:36 \u001b[01;34mshare\u001b[0m/\n",
            "drwxr-xr-x  2 root root 4096 Sep 21  2020 \u001b[01;34msrc\u001b[0m/\n",
            "drwxr-xr-x  2 root root 4096 May 13 13:46 \u001b[01;34mxgboost\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "okVgSiC5aidy",
        "outputId": "33bd26a6-da1d-4d55-ec3b-6a8d9075690f"
      },
      "source": [
        "!pip3 uninstall -y tensorflow\n",
        "!pip3 install 'tensorflow-gpu==1.15.4'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 1.15.4\n",
            "Uninstalling tensorflow-1.15.4:\n",
            "  Successfully uninstalled tensorflow-1.15.4\n",
            "Collecting tensorflow-gpu==1.15.4\n",
            "  Downloading tensorflow_gpu-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (411.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.0 MB 23 kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.38.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.15.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (3.17.0)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 257 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (0.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.4) (0.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (56.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.4) (3.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (4.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.4) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.4) (3.10.0.0)\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: deepspeech-training 0.9.3 requires tensorflow==1.15.4, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: moviepy 0.2.3.5 has requirement decorator<5.0,>=4.0.2, but you'll have decorator 5.0.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.16.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tensorflow-gpu\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.20.3\n",
            "    Uninstalling numpy-1.20.3:\n",
            "      Successfully uninstalled numpy-1.20.3\n",
            "Successfully installed numpy-1.18.5 tensorflow-gpu-1.15.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7I9AA5udz7l"
      },
      "source": [
        "# Building LM and scorer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WUqcQODzJC7",
        "outputId": "345ca007-c7bc-45b9-a07a-48b14653dd7a"
      },
      "source": [
        "%cd /content/DeepSpeech/kenlm/\n",
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
        "!cd kenlm\n",
        "!mkdir build\n",
        "!cmake kenlm\n",
        "!make -j 4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech/kenlm\n",
            "--2021-05-19 16:51:57--  https://kheafield.com/code/kenlm.tar.gz\n",
            "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
            "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 491090 (480K) [application/x-gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>] 479.58K  1.44MB/s    in 0.3s    \n",
            "\n",
            "2021-05-19 16:51:58 (1.44 MB/s) - written to stdout [491090/491090]\n",
            "\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   program_options\n",
            "--   system\n",
            "--   thread\n",
            "--   unit_test_framework\n",
            "--   chrono\n",
            "--   date_time\n",
            "--   atomic\n",
            "-- Check if compiler accepts -pthread\n",
            "-- Check if compiler accepts -pthread - yes\n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/include (found version \"5.2.2\") \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/DeepSpeech/kenlm\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_util\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 38%] Built target kenlm_util\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_filter\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target probing_hash_table_benchmark\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 56%] Built target kenlm_filter\n",
            "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 62%] Built target probing_hash_table_benchmark\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "\u001b[35m\u001b[1mScanning dependencies of target build_binary\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_benchmark\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fragment\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target query\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 78%] Built target fragment\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_builder\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 80%] Built target build_binary\n",
            "\u001b[35m\u001b[1mScanning dependencies of target phrase_table_vocab\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 82%] Built target query\n",
            "\u001b[35m\u001b[1mScanning dependencies of target filter\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 85%] Built target phrase_table_vocab\n",
            "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 92%] Built target kenlm_benchmark\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 93%] Built target filter\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 95%] Built target kenlm_builder\n",
            "\u001b[35m\u001b[1mScanning dependencies of target count_ngrams\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lmplz\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 98%] Built target lmplz\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXLxORYx1OOQ",
        "outputId": "8f2876ee-5046-40aa-c739-6ba00f5afaa8"
      },
      "source": [
        "%cd /content/DeepSpeech/\n",
        "!python3 lm_optimizer.py \\\n",
        "  --test_files /content/drive/MyDrive/CSVs/test.csv \\\n",
        "  --checkpoint_dir /content/drive/MyDrive/cache_diff_checkpoints/deepspeech-0.9.3-checkpoint/ \\\n",
        "  --alphabet_config_path /content/drive/MyDrive/alphabet-urdu.txt \\\n",
        "  --test_batch_size 8 \\\n",
        "  --n_trials 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech\n",
            "\u001b[32m[I 2021-05-20 18:28:18,827]\u001b[0m A new study created in memory with name: no-name-33b152d7-9916-48c3-8e54-d58d07faf685\u001b[0m\n",
            "I0520 18:28:19.054479 140540439070592 utils.py:157] NumExpr defaulting to 4 threads.\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/cache_diff_checkpoints/deepspeech-0.9.3-checkpoint/best_dev-1599195\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/drive/MyDrive/CSVs/test.csv\n",
            "Test epoch | Steps: 3 | Elapsed Time: 0:01:13                                   "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-R3zuY25klm",
        "outputId": "e87cc9fa-e8cf-46c9-9787-68bd5fde9d8c"
      },
      "source": [
        "%cd /content/DeepSpeech/data/lm/\n",
        "!python3 generate_lm.py \\\n",
        "  --input_txt /content/drive/MyDrive/ur.txt \\\n",
        "  --output_dir /content/drive/MyDrive/urdu_scorer \\\n",
        "  --top_k 500000 --kenlm_bins /content/DeepSpeech/kenlm/bin/ \\\n",
        "  --arpa_order 5 --max_arpa_memory \"85%\" --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 --binary_q_bits 8 --binary_type trie"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech/data/lm\n",
            "\n",
            "Converting to lowercase and counting word occurrences ...\n",
            "| |           #                                   | 683734 Elapsed Time: 0:00:10\n",
            "\n",
            "Saving top 500000 words ...\n",
            "\n",
            "Calculating word statistics ...\n",
            "  Your text file has 3612648 words in total\n",
            "  It has 186487 unique words\n",
            "  Your top-500000 words are 100.0000 percent of all words\n",
            "  Your most common word \"کے\" occurred 106973 times\n",
            "  The least common word in your top-k is \"یہہ\" with 1 times\n",
            "  The first word with 2 occurrences is \"یودھا\" at place 63516\n",
            "\n",
            "Creating ARPA file ...\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/drive/MyDrive/urdu_scorer/lower.txt.gz\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 4096499712 bytes == 0x55fab03e0000 @  0x7f5fb31d31e7 0x55faae4717a2 0x55faae40c51e 0x55faae3eb2eb 0x55faae3d7066 0x7f5fb136cbf7 0x55faae3d8baa\n",
            "tcmalloc: large alloc 19116990464 bytes == 0x55fba469a000 @  0x7f5fb31d31e7 0x55faae4717a2 0x55faae4607ca 0x55faae461208 0x55faae3eb308 0x55faae3d7066 0x7f5fb136cbf7 0x55faae3d8baa\n",
            "****************************************************************************************************\n",
            "Unigram tokens 3612648 types 186490\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:2237880 2:2266967296 3:4250563840 4:6800901632 5:9917982720\n",
            "tcmalloc: large alloc 9917988864 bytes == 0x55fab02d2000 @  0x7f5fb31d31e7 0x55faae4717a2 0x55faae4607ca 0x55faae461208 0x55faae3eb8d7 0x55faae3d7066 0x7f5fb136cbf7 0x55faae3d8baa\n",
            "tcmalloc: large alloc 2266972160 bytes == 0x55fcff786000 @  0x7f5fb31d31e7 0x55faae4717a2 0x55faae4607ca 0x55faae461208 0x55faae3ebcdd 0x55faae3d7066 0x7f5fb136cbf7 0x55faae3d8baa\n",
            "tcmalloc: large alloc 4250566656 bytes == 0x55fd8697a000 @  0x7f5fb31d31e7 0x55faae4717a2 0x55faae4607ca 0x55faae461208 0x55faae3ebcdd 0x55faae3d7066 0x7f5fb136cbf7 0x55faae3d8baa\n",
            "tcmalloc: large alloc 6800908288 bytes == 0x560018e90000 @  0x7f5fb31d31e7 0x55faae4717a2 0x55faae4607ca 0x55faae461208 0x55faae3ebcdd 0x55faae3d7066 0x7f5fb136cbf7 0x55faae3d8baa\n",
            "Statistics:\n",
            "1 186490 D1=0.777403 D2=1.05574 D3+=1.23754\n",
            "2 1146788 D1=0.796378 D2=1.0792 D3+=1.33647\n",
            "3 376825/2156625 D1=0.850617 D2=1.2574 D3+=1.44502\n",
            "4 254602/2296740 D1=0.912636 D2=1.34344 D3+=1.52928\n",
            "5 163738/2093416 D1=0.888446 D2=1.57923 D3+=1.50063\n",
            "Memory estimate for binary LM:\n",
            "type    MB\n",
            "probing 48 assuming -p 1.5\n",
            "probing 59 assuming -r models -p 1.5\n",
            "trie    26 without quantization\n",
            "trie    15 assuming -q 8 -b 8 quantization \n",
            "trie    23 assuming -a 22 array pointer compression\n",
            "trie    13 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:2237880 2:18348608 3:7536500 4:6110448 5:4584664\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "*******#############################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:2237880 2:18348608 3:7536500 4:6110448 5:4584664\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:29778524 kB\tVmRSS:4281128 kB\tRSSMax:4283304 kB\tuser:4.84255\tsys:2.76304\tCPU:7.60564\treal:7.72722\n",
            "\n",
            "Filtering ARPA file using vocabulary of top-k words ...\n",
            "Reading /content/drive/MyDrive/urdu_scorer/lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "\n",
            "Building lm.binary ...\n",
            "Reading /content/drive/MyDrive/urdu_scorer/lm_filtered.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Quantizing\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXLo8TSx9Aqw",
        "outputId": "b090105b-57a7-441f-9b04-1c360155aac6"
      },
      "source": [
        "!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/native_client.amd64.cuda.linux.tar.xz\n",
        "!tar -Jxvf native_client.amd64.cuda.linux.tar.xz -C /content/DeepSpeech/data/lm/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-19 16:58:05--  https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/native_client.amd64.cuda.linux.tar.xz\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/60273704/41d8a080-3b15-11eb-87d9-8c3ab703f1c3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210519%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210519T165806Z&X-Amz-Expires=300&X-Amz-Signature=dc239de87839089315f722bc505a641400e2396db7d2d68c896d44ae7e7079c4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Dnative_client.amd64.cuda.linux.tar.xz&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-05-19 16:58:06--  https://github-releases.githubusercontent.com/60273704/41d8a080-3b15-11eb-87d9-8c3ab703f1c3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210519%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210519T165806Z&X-Amz-Expires=300&X-Amz-Signature=dc239de87839089315f722bc505a641400e2396db7d2d68c896d44ae7e7079c4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Dnative_client.amd64.cuda.linux.tar.xz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.110.154, 185.199.111.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14086680 (13M) [application/octet-stream]\n",
            "Saving to: ‘native_client.amd64.cuda.linux.tar.xz’\n",
            "\n",
            "native_client.amd64 100%[===================>]  13.43M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-19 16:58:06 (117 MB/s) - ‘native_client.amd64.cuda.linux.tar.xz’ saved [14086680/14086680]\n",
            "\n",
            "libdeepspeech.so\n",
            "generate_scorer_package\n",
            "LICENSE\n",
            "deepspeech\n",
            "deepspeech.h\n",
            "README.mozilla\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMJAuFK5-BLM",
        "outputId": "59986ff7-3a91-4121-9930-1177fd9bafa6"
      },
      "source": [
        "%cd /content/DeepSpeech/data/lm/\n",
        "!./generate_scorer_package \\\n",
        "  --alphabet /content/drive/MyDrive/alphabet-urdu.txt  \\\n",
        "  --lm /content/drive/MyDrive/urdu_scorer/lm.binary \\\n",
        "  --vocab /content/drive/MyDrive/urdu_scorer/vocab-500000.txt \\\n",
        "  --package kenlm-urdu.scorer \\\n",
        "  --default_alpha 4.8448516894284746 \\\n",
        "  --default_beta 1.0815538413683652 \\\n",
        "  --force_bytes_output_mode True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech/data/lm\n",
            "186487 unique words read from vocabulary file.\n",
            "Doesn't look like a character based (Bytes Are All You Need) model.\n",
            "Package created in kenlm-urdu.scorer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgXrcxkRBZav",
        "outputId": "2d80bf11-3ac0-406a-edb3-17c0c0df83c8"
      },
      "source": [
        "%cd /content/DeepSpeech/data/lm/\n",
        "!cp kenlm-urdu.scorer /content/drive/MyDrive/urdu_scorer/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech/data/lm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy9QlAgTd8X5"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS7RTSIm2-mv"
      },
      "source": [
        "!python3 /content/DeepSpeech/training/deepspeech_training/util/check_characters.py -csv /content/train.csv -alpha\n",
        "!python3 /content/DeepSpeech/training/deepspeech_training/util/check_characters.py -csv /content/dev.csv -alpha\n",
        "!python3 /content/DeepSpeech/training/deepspeech_training/util/check_characters.py -csv /content/test.csv -alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHkyw8frbCV8",
        "outputId": "3d5ee367-2e84-4675-9c5f-27df41ac730b"
      },
      "source": [
        "%cd /content/DeepSpeech\n",
        "!TF_CUDNN_RESET_RND_GEN_STATE=1 python3 DeepSpeech.py --train_cudnn True \\\n",
        "    --epochs 200 \\\n",
        "    --reduce_lr_on_plateau True --plateau_epochs 7 \\\n",
        "    --learning_rate 0.001 --dropout_rate 0.2 \\\n",
        "    --alphabet_config_path /content/drive/MyDrive/alphabet-urdu.txt \\\n",
        "    --checkpoint_dir /content/drive/MyDrive/scratch_model/ \\\n",
        "    --export_dir /content/drive/MyDrive/models/ --export_file_name 'urdu_model' \\\n",
        "    --train_files /content/drive/MyDrive/CSVs/train.csv \\\n",
        "    --dev_files /content/drive/MyDrive/CSVs/dev.csv \\\n",
        "    --test_files /content/drive/MyDrive/CSVs/test.csv \\\n",
        "    --scorer_path /content/drive/MyDrive/urdu_scorer/kenlm-urdu.scorer \\\n",
        "    --max_to_keep 1 \\\n",
        "    --train_batch_size 8 \\\n",
        "    --dev_batch_size 8 \\\n",
        "    --test_batch_size 8 \\\n",
        "    --augment frequency_mask[p=0.8,n=2:4,size=2:4] \\\n",
        "    --augment time_mask[p=0.8,n=2:4,size=10:50,domain=spectrogram]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech\n",
            "I0521 02:41:44.117775 139763795343232 utils.py:157] NumExpr defaulting to 4 threads.\n",
            "I Could not find best validating checkpoint.\n",
            "I Could not find most recent checkpoint.\n",
            "I Initializing all variables.\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:22:35 | Steps: 1580 | Loss: 351.943948   \n",
            "Epoch 0 | Validation | Elapsed Time: 0:01:41 | Steps: 106 | Loss: 444.072560 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 444.072560 to: /content/drive/MyDrive/scratch_model/best_dev-1580\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 272.622852   \n",
            "Epoch 1 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 367.671437 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 367.671437 to: /content/drive/MyDrive/scratch_model/best_dev-3160\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 245.239147   \n",
            "Epoch 2 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 327.530785 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 327.530785 to: /content/drive/MyDrive/scratch_model/best_dev-4740\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 |   Training | Elapsed Time: 0:02:53 | Steps: 1580 | Loss: 231.602420   \n",
            "Epoch 3 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 353.741430 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 222.870052   \n",
            "Epoch 4 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 305.753621 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 305.753621 to: /content/drive/MyDrive/scratch_model/best_dev-7900\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 215.515294   \n",
            "Epoch 5 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 302.985393 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 302.985393 to: /content/drive/MyDrive/scratch_model/best_dev-9480\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 |   Training | Elapsed Time: 0:02:53 | Steps: 1580 | Loss: 210.179942   \n",
            "Epoch 6 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 298.752264 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 298.752264 to: /content/drive/MyDrive/scratch_model/best_dev-11060\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 206.050548   \n",
            "Epoch 7 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 293.106190 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 293.106190 to: /content/drive/MyDrive/scratch_model/best_dev-12640\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 202.870458   \n",
            "Epoch 8 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 291.480309 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 291.480309 to: /content/drive/MyDrive/scratch_model/best_dev-14220\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 200.027097   \n",
            "Epoch 9 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 282.847556 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 282.847556 to: /content/drive/MyDrive/scratch_model/best_dev-15800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 197.783627  \n",
            "Epoch 10 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 278.397995 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 278.397995 to: /content/drive/MyDrive/scratch_model/best_dev-17380\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 195.650928  \n",
            "Epoch 11 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 282.754542 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 194.008994  \n",
            "Epoch 12 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 279.058595 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 191.977921  \n",
            "Epoch 13 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 278.284034 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 278.284034 to: /content/drive/MyDrive/scratch_model/best_dev-22120\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 190.741419  \n",
            "Epoch 14 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 278.842236 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 189.161848  \n",
            "Epoch 15 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 272.182485 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 272.182485 to: /content/drive/MyDrive/scratch_model/best_dev-25280\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 188.248156  \n",
            "Epoch 16 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 272.756152 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 187.908087  \n",
            "Epoch 17 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 267.974493 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 267.974493 to: /content/drive/MyDrive/scratch_model/best_dev-28440\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 186.786779  \n",
            "Epoch 18 | Validation | Elapsed Time: 0:00:13 | Steps: 106 | Loss: 260.891689 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 260.891689 to: /content/drive/MyDrive/scratch_model/best_dev-30020\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19 |   Training | Elapsed Time: 0:02:53 | Steps: 1580 | Loss: 186.984519  \n",
            "Epoch 19 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 261.043286 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 185.861942  \n",
            "Epoch 20 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 255.828978 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 255.828978 to: /content/drive/MyDrive/scratch_model/best_dev-33180\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 185.526095  \n",
            "Epoch 21 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 259.739724 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 185.324591  \n",
            "Epoch 22 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 262.666735 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23 |   Training | Elapsed Time: 0:02:53 | Steps: 1580 | Loss: 184.282473  \n",
            "Epoch 23 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 258.112526 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 184.020463  \n",
            "Epoch 24 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 254.256855 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 254.256855 to: /content/drive/MyDrive/scratch_model/best_dev-39500\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 183.427962  \n",
            "Epoch 25 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 251.153290 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 251.153290 to: /content/drive/MyDrive/scratch_model/best_dev-41080\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 26 |   Training | Elapsed Time: 0:02:53 | Steps: 1580 | Loss: 182.715968  \n",
            "Epoch 26 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 252.347017 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 27 |   Training | Elapsed Time: 0:02:53 | Steps: 1580 | Loss: 183.740320  \n",
            "Epoch 27 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 251.760955 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 28 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 182.233710  \n",
            "Epoch 28 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 243.485957 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 243.485957 to: /content/drive/MyDrive/scratch_model/best_dev-45820\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 29 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 181.867332  \n",
            "Epoch 29 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 249.097058 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 30 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 181.600948  \n",
            "Epoch 30 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 248.037720 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 31 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 182.229828  \n",
            "Epoch 31 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 246.750167 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 32 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 183.135801  \n",
            "Epoch 32 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 249.379755 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 33 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 182.024079  \n",
            "Epoch 33 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 245.928033 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 34 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 181.336933  \n",
            "Epoch 34 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 243.870599 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 35 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 181.500950  \n",
            "Epoch 35 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 245.966570 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/scratch_model/best_dev-45820\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/opaque_kernel\n",
            "I Loading variable from checkpoint: cudnn_lstm/opaque_kernel/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/opaque_kernel/Adam_1\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/bias/Adam\n",
            "I Loading variable from checkpoint: layer_6/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Loading variable from checkpoint: layer_6/weights/Adam\n",
            "I Loading variable from checkpoint: layer_6/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I Encountered a plateau, reducing learning rate to 0.00010000000474974513\n",
            "I Saved best validating model with reduced learning rate to: /content/drive/MyDrive/scratch_model/best_dev-45820\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 36 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 178.811672  \n",
            "Epoch 36 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 227.322104 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 227.322104 to: /content/drive/MyDrive/scratch_model/best_dev-47400\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 37 |   Training | Elapsed Time: 0:02:53 | Steps: 1580 | Loss: 173.213941  \n",
            "Epoch 37 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 227.698872 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 38 |   Training | Elapsed Time: 0:02:53 | Steps: 1580 | Loss: 170.807458  \n",
            "Epoch 38 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 225.090091 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 225.090091 to: /content/drive/MyDrive/scratch_model/best_dev-50560\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 39 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 169.234333  \n",
            "Epoch 39 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 225.035320 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 225.035320 to: /content/drive/MyDrive/scratch_model/best_dev-52140\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 40 |   Training | Elapsed Time: 0:02:53 | Steps: 1580 | Loss: 167.804064  \n",
            "Epoch 40 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 224.231100 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 224.231100 to: /content/drive/MyDrive/scratch_model/best_dev-53720\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 41 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 166.880521  \n",
            "Epoch 41 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 223.576383 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 223.576383 to: /content/drive/MyDrive/scratch_model/best_dev-55300\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 42 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 165.770706  \n",
            "Epoch 42 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 222.058240 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 222.058240 to: /content/drive/MyDrive/scratch_model/best_dev-56880\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 43 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 164.971853  \n",
            "Epoch 43 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 221.735000 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 221.735000 to: /content/drive/MyDrive/scratch_model/best_dev-58460\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 44 |   Training | Elapsed Time: 0:02:55 | Steps: 1580 | Loss: 164.289096  \n",
            "Epoch 44 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 221.161643 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 221.161643 to: /content/drive/MyDrive/scratch_model/best_dev-60040\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 45 |   Training | Elapsed Time: 0:02:54 | Steps: 1580 | Loss: 163.624235  \n",
            "Epoch 45 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 221.485537 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 46 |   Training | Elapsed Time: 0:02:56 | Steps: 1580 | Loss: 162.767217  \n",
            "Epoch 46 | Validation | Elapsed Time: 0:00:12 | Steps: 106 | Loss: 220.771095 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "I Saved new best validating model with loss 220.771095 to: /content/drive/MyDrive/scratch_model/best_dev-63200\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 47 |   Training | Elapsed Time: 0:02:56 | Steps: 1580 | Loss: 162.414754  \n",
            "Epoch 47 | Validation | Elapsed Time: 0:00:13 | Steps: 106 | Loss: 221.525351 | Dataset: /content/drive/MyDrive/CSVs/dev.csv\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 48 |   Training | Elapsed Time: 0:00:49 | Steps: 585 | Loss: 136.390424   Process ForkPoolWorker-388:\n",
            "Process ForkPoolWorker-387:\n",
            "Process ForkPoolWorker-386:\n",
            "Process ForkPoolWorker-385:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "I FINISHED optimization in 2:52:04.466165\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/scratch_model/best_dev-63200\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/drive/MyDrive/CSVs/test.csv\n",
            "Test epoch | Steps: 1 | Elapsed Time: 0:00:02                                   Process ForkPoolWorker-395:\n",
            "Process ForkPoolWorker-391:\n",
            "Process ForkPoolWorker-390:\n",
            "Process ForkPoolWorker-389:\n",
            "Process ForkPoolWorker-394:\n",
            "Process ForkPoolWorker-393:\n",
            "Process ForkPoolWorker-396:\n",
            "Process ForkPoolWorker-392:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKOZ1lPsCIDZ"
      },
      "source": [
        "# Converting to pbmm format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmPA3Pl_CKgI",
        "outputId": "8078c203-d035-438a-9c87-7ef1ee44faf9"
      },
      "source": [
        "# converting pb to pbmm\n",
        "%cd /content\n",
        "!python3 /content/DeepSpeech/util/taskcluster.py --source tensorflow --artifact convert_graphdef_memmapped_format --branch r1.15 --target .\n",
        "!/content/convert_graphdef_memmapped_format --in_graph=/content/drive/MyDrive/models/more_data_model.pb --out_graph=/content/drive/MyDrive/models/more_data_model.pbmm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.tensorflow.pip.r1.15.cpu/artifacts/public/convert_graphdef_memmapped_format ...\n",
            "Downloading: 100%\n",
            "\n",
            "2021-05-20 18:06:32.755725: I tensorflow/contrib/util/convert_graphdef_memmapped_format_lib.cc:171] Converted 7 nodes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}